{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1f65bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./config/parameters.py\n",
    "%run ./config/path.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcd8d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "from sklearn import metrics\n",
    "from catboost import CatBoostClassifier\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd7dca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(P_TRAIN_DATA)\n",
    "test_data = pd.read_csv(P_TEST_DATA )\n",
    "sample_submission = pd.read_csv(P_SAMPLE_SUB)\n",
    "\n",
    "# Target Preprocessing\n",
    "hr_data = train_data.astype({'Loan_Status': 'int'})\n",
    "\n",
    "# Constants Definition\n",
    "TARGET_VAR = 'Loan_Status'\n",
    "# Define the new column order\n",
    "new_column_order = [\n",
    "    'Gender', 'Married', 'Dependents', 'Education',\n",
    "    'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome',\n",
    "    'LoanAmount', 'Credit_History', 'Property_Area',\n",
    "    'Total_Income', 'Loan_Amount_Term'\n",
    "]\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "#train_data = train_data[new_column_order + ['Loan_Status']]\n",
    "\n",
    "# Now your feature columns (excluding IDs and target) are:\n",
    "FEATURE_COLUMNS = new_column_order\n",
    "\n",
    "# Removing Constant Value Features\n",
    "# FEATURE_COLUMNS.remove('Over18')\n",
    "# FEATURE_COLUMNS.remove('StandardHours')\n",
    "# FEATURE_COLUMNS.remove('EmployeeCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "072af9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_data = train_data.loc[:, FEATURE_COLUMNS + [TARGET_VAR]].copy()\n",
    "processed_test_data = test_data.loc[:, FEATURE_COLUMNS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b38439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 1: Gender-Credit Risk Factor\n",
    "processed_train_data['female_credit_risk'] = (\n",
    "    (processed_train_data['Gender'] == 0) & \n",
    "    (processed_train_data['Credit_History'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 2: Education Paradox Risk\n",
    "processed_train_data['educated_risk_paradox'] = (\n",
    "    (processed_train_data['Education'] == 1) & \n",
    "    (processed_train_data['Credit_History'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 3: Married Credit Risk Interaction\n",
    "processed_train_data['married_credit_risk'] = (\n",
    "    (processed_train_data['Married'] == 1) & \n",
    "    (processed_train_data['Credit_History'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 4: Urban Premium Risk\n",
    "processed_train_data['urban_advantage'] = (\n",
    "    (processed_train_data['Property_Area'] == 2) & \n",
    "    (processed_train_data['Credit_History'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 5: Suburban Safe Haven\n",
    "processed_train_data['suburban_safe_haven'] = (\n",
    "    (processed_train_data['Property_Area'] == 1) & \n",
    "    (processed_train_data['Credit_History'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 6: High Dependents Bonus\n",
    "processed_train_data['high_dependents_boost'] = (\n",
    "    (processed_train_data['Dependents'] == '3+') & \n",
    "    (processed_train_data['Credit_History'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 7: Self-Employment Paradox\n",
    "processed_train_data['self_employed_risk'] = (\n",
    "    (processed_train_data['Self_Employed'] == 1) & \n",
    "    (processed_train_data['Credit_History'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# First categorize loan terms (as previously defined)\n",
    "processed_train_data['Loan_Term_Category'] = processed_train_data['Loan_Amount_Term'].apply(\n",
    "    lambda x: 'short' if x <= 100 else ('medium' if x <= 200 else 'long'))\n",
    "\n",
    "# Feature 8: Medium Term High Risk\n",
    "processed_train_data['medium_term_high_risk'] = (\n",
    "    (processed_train_data['Loan_Term_Category'] == 'medium') & \n",
    "    (processed_train_data['Credit_History'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 9: Double Income Vulnerability\n",
    "processed_train_data['low_income_high_risk'] = (\n",
    "    (processed_train_data['ApplicantIncome'] < processed_train_data['ApplicantIncome'].median()) & \n",
    "    (processed_train_data['CoapplicantIncome'] == 0) & \n",
    "    (processed_train_data['Credit_History'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 10: Approval Probability Estimator\n",
    "conditions = [\n",
    "    (processed_train_data['Credit_History'] == 1),\n",
    "    (processed_train_data['Property_Area'] == 1),\n",
    "    (processed_train_data['Dependents'] == '3+'),\n",
    "    (processed_train_data['Loan_Term_Category'] == 'medium')\n",
    "]\n",
    "\n",
    "choices = [0.83, 0.85, 0.91, 1.00]  # Approval probabilities from insights\n",
    "processed_train_data['composite_approval_score'] = np.select(conditions, choices, default=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f04c1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 1: Gender-Credit Risk Factor\n",
    "processed_test_data['female_credit_risk'] = (\n",
    "    (processed_test_data['Gender'] == 0) & \n",
    "    (processed_test_data['Credit_History'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 2: Education Paradox Risk\n",
    "processed_test_data['educated_risk_paradox'] = (\n",
    "    (processed_test_data['Education'] == 1) & \n",
    "    (processed_test_data['Credit_History'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 3: Married Credit Risk Interaction\n",
    "processed_test_data['married_credit_risk'] = (\n",
    "    (processed_test_data['Married'] == 1) & \n",
    "    (processed_test_data['Credit_History'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 4: Urban Premium Risk\n",
    "processed_test_data['urban_advantage'] = (\n",
    "    (processed_test_data['Property_Area'] == 2) & \n",
    "    (processed_test_data['Credit_History'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 5: Suburban Safe Haven\n",
    "processed_test_data['suburban_safe_haven'] = (\n",
    "    (processed_test_data['Property_Area'] == 1) & \n",
    "    (processed_test_data['Credit_History'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 6: High Dependents Bonus\n",
    "processed_test_data['high_dependents_boost'] = (\n",
    "    (processed_test_data['Dependents'] == '3+') & \n",
    "    (processed_test_data['Credit_History'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 7: Self-Employment Paradox\n",
    "processed_test_data['self_employed_risk'] = (\n",
    "    (processed_test_data['Self_Employed'] == 1) & \n",
    "    (processed_test_data['Credit_History'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# First categorize loan terms (as previously defined)\n",
    "processed_test_data['Loan_Term_Category'] = processed_test_data['Loan_Amount_Term'].apply(\n",
    "    lambda x: 'short' if x <= 100 else ('medium' if x <= 200 else 'long'))\n",
    "\n",
    "# Feature 8: Medium Term High Risk\n",
    "processed_test_data['medium_term_high_risk'] = (\n",
    "    (processed_test_data['Loan_Term_Category'] == 'medium') & \n",
    "    (processed_test_data['Credit_History'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 9: Double Income Vulnerability\n",
    "processed_test_data['low_income_high_risk'] = (\n",
    "    (processed_test_data['ApplicantIncome'] < processed_test_data['ApplicantIncome'].median()) & \n",
    "    (processed_test_data['CoapplicantIncome'] == 0) & \n",
    "    (processed_test_data['Credit_History'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Feature 10: Approval Probability Estimator\n",
    "conditions = [\n",
    "    (processed_test_data['Credit_History'] == 1),\n",
    "    (processed_test_data['Property_Area'] == 1),\n",
    "    (processed_test_data['Dependents'] == '3+'),\n",
    "    (processed_test_data['Loan_Term_Category'] == 'medium')\n",
    "]\n",
    "\n",
    "choices = [0.83, 0.85, 0.91, 1.00]  # Approval probabilities from insights\n",
    "processed_test_data['composite_approval_score'] = np.select(conditions, choices, default=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60410c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_data.drop(columns=['Loan_Amount_Term'], inplace=True)\n",
    "processed_train_data.drop(columns=['Loan_Amount_Term'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6e155a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, value in enumerate(ORDINAL_CAT_ORDER):\n",
    "    processed_train_data.loc[processed_train_data['Dependents'] == value, 'Dependents'] = idx\n",
    "    processed_test_data.loc[processed_test_data['Dependents'] == value, 'Dependents'] = idx\n",
    "\n",
    "processed_train_data = processed_train_data.astype({'Dependents': 'int'})\n",
    "processed_test_data = processed_test_data.astype({'Dependents': 'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5d0b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_train_data[BINARY_FEATURES] = processed_test_data[BINARY_FEATURES].astype(int)\n",
    "# processed_test_data[BINARY_FEATURES] = processed_test_data[BINARY_FEATURES].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2b58b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_train_data[CATEGORICAL_FEATURES] = processed_test_data[CATEGORICAL_FEATURES].astype(int)\n",
    "# processed_test_data[CATEGORICAL_FEATURES] = processed_test_data[CATEGORICAL_FEATURES].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f1db384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_train_data = processed_train_data.drop(columns=['Loan_Status'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b575cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cont_feature in CONTINUOUS_FEATURES:\n",
    "    mean_value = np.mean(processed_train_data[cont_feature])\n",
    "    std_dev = np.std(processed_train_data[cont_feature])\n",
    "    \n",
    "    processed_train_data[cont_feature] = (processed_train_data[cont_feature] - mean_value) / std_dev\n",
    "    \n",
    "    processed_test_data[cont_feature] = (processed_test_data[cont_feature] - mean_value) / std_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de01350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_FEATURES = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed',\n",
    "       'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Credit_History', \n",
    "       'Total_Income', 'female_credit_risk', 'Property_Area',\n",
    "       'educated_risk_paradox', 'married_credit_risk', 'urban_advantage',\n",
    "       'suburban_safe_haven', 'high_dependents_boost', 'self_employed_risk', \n",
    "       'medium_term_high_risk', 'low_income_high_risk',\n",
    "       'composite_approval_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d94b36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize encoder (fit only on train data)\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, dtype=int)\n",
    "ohe.fit(processed_train_data[['Loan_Term_Category']])  # Fit only on train\n",
    "\n",
    "# Transform both datasets\n",
    "train_encoded = ohe.transform(processed_train_data[['Loan_Term_Category']])\n",
    "test_encoded = ohe.transform(processed_test_data[['Loan_Term_Category']])\n",
    "\n",
    "# Get feature names (e.g., ['Loan_Term_Category_medium', 'Loan_Term_Category_long'])\n",
    "encoded_cols = ohe.get_feature_names_out(['Loan_Term_Category'])\n",
    "\n",
    "# Convert to DataFrames\n",
    "train_loan_term_df = pd.DataFrame(train_encoded, columns=encoded_cols, index=processed_train_data.index)\n",
    "test_loan_term_df = pd.DataFrame(test_encoded, columns=encoded_cols, index=processed_test_data.index)\n",
    "\n",
    "# Concatenate and drop original column\n",
    "processed_train_data = pd.concat([processed_train_data, train_loan_term_df], axis=1).drop('Loan_Term_Category', axis=1)\n",
    "processed_test_data = pd.concat([processed_test_data, test_loan_term_df], axis=1).drop('Loan_Term_Category', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cae109e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>auc</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>eta</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>skf_seed</th>\n",
       "      <th>xgb_seed</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.513658</td>\n",
       "      <td>100</td>\n",
       "      <td>0.7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1054</td>\n",
       "      <td>1517</td>\n",
       "      <td>0.733639</td>\n",
       "      <td>0.833134</td>\n",
       "      <td>0.850600</td>\n",
       "      <td>0.841777</td>\n",
       "      <td>[[ 148.  837.]\\n [ 734. 4179.]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.508381</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1054</td>\n",
       "      <td>1731</td>\n",
       "      <td>0.804510</td>\n",
       "      <td>0.832508</td>\n",
       "      <td>0.958070</td>\n",
       "      <td>0.890887</td>\n",
       "      <td>[[  38.  947.]\\n [ 206. 4707.]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.504940</td>\n",
       "      <td>25</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1054</td>\n",
       "      <td>460</td>\n",
       "      <td>0.760258</td>\n",
       "      <td>0.835346</td>\n",
       "      <td>0.887034</td>\n",
       "      <td>0.860415</td>\n",
       "      <td>[[ 126.  859.]\\n [ 555. 4358.]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.502987</td>\n",
       "      <td>250</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1054</td>\n",
       "      <td>1128</td>\n",
       "      <td>0.802815</td>\n",
       "      <td>0.831624</td>\n",
       "      <td>0.957053</td>\n",
       "      <td>0.889940</td>\n",
       "      <td>[[  33.  952.]\\n [ 211. 4702.]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.502754</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1054</td>\n",
       "      <td>745</td>\n",
       "      <td>0.827569</td>\n",
       "      <td>0.832764</td>\n",
       "      <td>0.992265</td>\n",
       "      <td>0.905545</td>\n",
       "      <td>[[   6.  979.]\\n [  38. 4875.]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.502272</td>\n",
       "      <td>200</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1054</td>\n",
       "      <td>1705</td>\n",
       "      <td>0.777043</td>\n",
       "      <td>0.831430</td>\n",
       "      <td>0.918583</td>\n",
       "      <td>0.872836</td>\n",
       "      <td>[[  70.  915.]\\n [ 400. 4513.]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1054</td>\n",
       "      <td>1921</td>\n",
       "      <td>0.832994</td>\n",
       "      <td>0.832994</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908889</td>\n",
       "      <td>[[   0.  985.]\\n [   0. 4913.]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.498385</td>\n",
       "      <td>200</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1054</td>\n",
       "      <td>1406</td>\n",
       "      <td>0.829773</td>\n",
       "      <td>0.832794</td>\n",
       "      <td>0.995522</td>\n",
       "      <td>0.906916</td>\n",
       "      <td>[[3.000e+00 9.820e+02]\\n [2.200e+01 4.891e+03]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.488566</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1054</td>\n",
       "      <td>983</td>\n",
       "      <td>0.832994</td>\n",
       "      <td>0.832994</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908889</td>\n",
       "      <td>[[   0.  985.]\\n [   0. 4913.]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.479672</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1054</td>\n",
       "      <td>1694</td>\n",
       "      <td>0.832146</td>\n",
       "      <td>0.832966</td>\n",
       "      <td>0.998779</td>\n",
       "      <td>0.908367</td>\n",
       "      <td>[[1.000e+00 9.840e+02]\\n [6.000e+00 4.907e+03]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step       auc  n_estimators  eta  max_depth  subsample  colsample_bytree  \\\n",
       "2     2  0.513658           100  0.7         10       0.25              0.50   \n",
       "4     4  0.508381            50  0.5          8       0.90              0.50   \n",
       "1     1  0.504940            25  0.9         10       0.25              0.25   \n",
       "9     9  0.502987           250  0.2          6       0.75              0.90   \n",
       "7     7  0.502754           200  0.1         10       0.90              0.25   \n",
       "3     3  0.502272           200  0.5         10       0.75              0.75   \n",
       "6     6  0.500000            25  0.0          4       0.90              0.50   \n",
       "5     5  0.498385           200  0.4          2       0.90              0.90   \n",
       "0     0  0.488566           100  0.2          2       0.50              0.50   \n",
       "8     8  0.479672            25  0.2          6       0.90              0.90   \n",
       "\n",
       "   skf_seed  xgb_seed  accuracy  precision    recall        f1  \\\n",
       "2      1054      1517  0.733639   0.833134  0.850600  0.841777   \n",
       "4      1054      1731  0.804510   0.832508  0.958070  0.890887   \n",
       "1      1054       460  0.760258   0.835346  0.887034  0.860415   \n",
       "9      1054      1128  0.802815   0.831624  0.957053  0.889940   \n",
       "7      1054       745  0.827569   0.832764  0.992265  0.905545   \n",
       "3      1054      1705  0.777043   0.831430  0.918583  0.872836   \n",
       "6      1054      1921  0.832994   0.832994  1.000000  0.908889   \n",
       "5      1054      1406  0.829773   0.832794  0.995522  0.906916   \n",
       "0      1054       983  0.832994   0.832994  1.000000  0.908889   \n",
       "8      1054      1694  0.832146   0.832966  0.998779  0.908367   \n",
       "\n",
       "                                  confusion_matrix  \n",
       "2                  [[ 148.  837.]\\n [ 734. 4179.]]  \n",
       "4                  [[  38.  947.]\\n [ 206. 4707.]]  \n",
       "1                  [[ 126.  859.]\\n [ 555. 4358.]]  \n",
       "9                  [[  33.  952.]\\n [ 211. 4702.]]  \n",
       "7                  [[   6.  979.]\\n [  38. 4875.]]  \n",
       "3                  [[  70.  915.]\\n [ 400. 4513.]]  \n",
       "6                  [[   0.  985.]\\n [   0. 4913.]]  \n",
       "5  [[3.000e+00 9.820e+02]\\n [2.200e+01 4.891e+03]]  \n",
       "0                  [[   0.  985.]\\n [   0. 4913.]]  \n",
       "8  [[1.000e+00 9.840e+02]\\n [6.000e+00 4.907e+03]]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tuning parameters\n",
    "n_estimators_values = [10, 25, 50, 100, 150, 200, 250, 300]\n",
    "eta_values = [v / 10 for v in range(10)]\n",
    "max_depth_values = [2, 4, 6, 8, 10]\n",
    "subsample_values = [0.25, 0.50, 0.75, 0.90]\n",
    "colsample_bytree_values = [0.25, 0.50, 0.75, 0.90]\n",
    "\n",
    "cv_folds = 10\n",
    "tuning_iterations = 10\n",
    "include_orig = True\n",
    "tuning_results = defaultdict(list)\n",
    "\n",
    "# Create column names for predictions\n",
    "col_names = [f'XGB_Step_{step}_Fold_{fold}' \n",
    "             for step in range(tuning_iterations) \n",
    "             for fold in range(cv_folds)]\n",
    "test_predictions = pd.DataFrame(0, index=processed_test_data.index, columns=col_names)\n",
    "valid_predictions = pd.DataFrame(0, index=processed_train_data.index, columns=col_names)\n",
    "\n",
    "# Initialize confusion matrix storage\n",
    "confusion_matrices = []\n",
    "\n",
    "random.seed(2201020)\n",
    "\n",
    "# Stratified K-Fold for Cross-Validation\n",
    "skf_seed = random.randint(0, 2023)\n",
    "skf = StratifiedKFold(n_splits=cv_folds, random_state=skf_seed, shuffle=True)\n",
    "\n",
    "# Model Tuning Loop\n",
    "for step in range(tuning_iterations):\n",
    "    n_estimators = random.choice(n_estimators_values)\n",
    "    eta = random.choice(eta_values)\n",
    "    max_depth = random.choice(max_depth_values)\n",
    "    subsample = random.choice(subsample_values)\n",
    "    colsample_bytree = random.choice(colsample_bytree_values)\n",
    "    \n",
    "    aucs = []\n",
    "    step_cm = np.zeros((2, 2))  # Initialize confusion matrix for this step\n",
    "\n",
    "    for i, (train_index, val_index) in enumerate(skf.split(processed_train_data[SELECTED_FEATURES], processed_train_data[TARGET_VAR])):\n",
    "        X_train, X_val = processed_train_data[SELECTED_FEATURES].iloc[train_index], processed_train_data[SELECTED_FEATURES].iloc[val_index]\n",
    "        y_train, y_val = processed_train_data[TARGET_VAR].iloc[train_index], processed_train_data[TARGET_VAR].iloc[val_index]\n",
    "        \n",
    "        xgb_seed = random.randint(0, 2023)\n",
    "        xgb = XGBClassifier(n_estimators=n_estimators, eta=eta, max_depth=max_depth, \n",
    "                           subsample=subsample, colsample_bytree=colsample_bytree, \n",
    "                           random_state=xgb_seed).fit(X_train.values, y_train)\n",
    "        \n",
    "        val_probs = [probs[1] for probs in xgb.predict_proba(X_val)]\n",
    "        val_preds = [1 if p >= 0.5 else 0 for p in val_probs]  # Binary predictions at 0.5 threshold\n",
    "        \n",
    "        valid_predictions.loc[val_index, f'XGB_Step_{step}_Fold_{i}'] = val_probs\n",
    "        \n",
    "        # Calculate metrics\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_val, val_probs, pos_label=1)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        aucs.append(auc)\n",
    "        \n",
    "        # Update confusion matrix for this step\n",
    "        cm = metrics.confusion_matrix(y_val, val_preds)\n",
    "        step_cm += cm\n",
    "        \n",
    "        test_predictions[f'XGB_Step_{step}_Fold_{i}'] = [probs[1] for probs in xgb.predict_proba(processed_test_data[SELECTED_FEATURES])]\n",
    "    \n",
    "    # Store confusion matrix for this step (sum across all folds)\n",
    "    confusion_matrices.append(step_cm)\n",
    "    \n",
    "    # Calculate performance metrics from confusion matrix\n",
    "    tn, fp, fn, tp = step_cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Storing the tuning results\n",
    "    tuning_results['step'].append(step)\n",
    "    tuning_results['auc'].append(mean(aucs))\n",
    "    tuning_results['n_estimators'].append(n_estimators)\n",
    "    tuning_results['eta'].append(eta)\n",
    "    tuning_results['max_depth'].append(max_depth)\n",
    "    tuning_results['subsample'].append(subsample)\n",
    "    tuning_results['colsample_bytree'].append(colsample_bytree)\n",
    "    tuning_results['skf_seed'].append(skf_seed)\n",
    "    tuning_results['xgb_seed'].append(xgb_seed)\n",
    "    tuning_results['accuracy'].append(accuracy)\n",
    "    tuning_results['precision'].append(precision)\n",
    "    tuning_results['recall'].append(recall)\n",
    "    tuning_results['f1'].append(f1)\n",
    "    tuning_results['confusion_matrix'].append(str(step_cm))\n",
    "    \n",
    "    # print(f'Step: {step}  AUC: {mean(aucs):.4f}  Accuracy: {accuracy:.4f}  Precision: {precision:.4f}  Recall: {recall:.4f}  F1: {f1:.4f}')\n",
    "    # print(f'Confusion Matrix:\\n{step_cm}\\n')\n",
    "\n",
    "# Saving Predictions and Tuning Results\n",
    "valid_predictions.to_csv('XGBoost_Valid_Predictions.csv', index=False)\n",
    "test_predictions.to_csv('XGBoost_Test_Predictions.csv', index=False)\n",
    "\n",
    "# Finalizing Tuning Results\n",
    "tuning_results = pd.DataFrame(tuning_results)\n",
    "tuning_results.sort_values(by='auc', ascending=False, inplace=True)\n",
    "tuning_results.to_csv('XGBoost_Tuning_Results.csv', index=False)\n",
    "\n",
    "# Save confusion matrices separately\n",
    "confusion_df = pd.DataFrame({\n",
    "    'step': range(tuning_iterations),\n",
    "    'confusion_matrix': [str(cm) for cm in confusion_matrices],\n",
    "    'true_negative': [cm[0, 0] for cm in confusion_matrices],\n",
    "    'false_positive': [cm[0, 1] for cm in confusion_matrices],\n",
    "    'false_negative': [cm[1, 0] for cm in confusion_matrices],\n",
    "    'true_positive': [cm[1, 1] for cm in confusion_matrices]\n",
    "})\n",
    "confusion_df.to_csv('XGBoost_Confusion_Matrices.csv', index=False)\n",
    "\n",
    "# Return the top performing models\n",
    "tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "908ae495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define tuning parameters\n",
    "# n_estimators_values = [10, 25, 50, 100, 150, 200, 250, 300]\n",
    "# eta_values = [v / 10 for v in range(10)]\n",
    "# max_depth_values = [2, 4, 6, 8, 10]\n",
    "# subsample_values = [0.25, 0.50, 0.75, 0.90]\n",
    "# colsample_bytree_values = [0.25, 0.50, 0.75, 0.90]\n",
    "\n",
    "# cv_folds = 10\n",
    "# tuning_iterations = 10\n",
    "# include_orig = True\n",
    "# tuning_results = defaultdict(list)\n",
    "\n",
    "# # Create column names for predictions\n",
    "# col_names = [f'XGB_Step_{step}_Fold_{fold}' \n",
    "#              for step in range(tuning_iterations) \n",
    "#              for fold in range(cv_folds)]\n",
    "# test_predictions = pd.DataFrame(0, index = processed_test_data.index, columns = col_names)\n",
    "# valid_predictions = pd.DataFrame(0, index = processed_train_data.index, columns = col_names)\n",
    "\n",
    "# random.seed(2201020)\n",
    "\n",
    "# # Stratified K-Fold for Cross-Validation\n",
    "# skf_seed = random.randint(0, 2023)\n",
    "# skf = StratifiedKFold(n_splits = cv_folds, random_state = skf_seed, shuffle = True)\n",
    "\n",
    "# # Model Tuning Loop\n",
    "# for step in range(tuning_iterations):\n",
    "#     n_estimators = random.choice(n_estimators_values)\n",
    "#     eta = random.choice(eta_values)\n",
    "#     max_depth = random.choice(max_depth_values)\n",
    "#     subsample = random.choice(subsample_values)\n",
    "#     colsample_bytree = random.choice(colsample_bytree_values)\n",
    "    \n",
    "#     aucs = []\n",
    "\n",
    "#     for i, (train_index, val_index) in enumerate(skf.split(processed_train_data[SELECTED_FEATURES], processed_train_data[TARGET_VAR])):\n",
    "#         X_train, X_val = processed_train_data[SELECTED_FEATURES].iloc[train_index], processed_train_data[SELECTED_FEATURES].iloc[val_index]\n",
    "#         y_train, y_val = processed_train_data[TARGET_VAR].iloc[train_index], processed_train_data[TARGET_VAR].iloc[val_index]\n",
    "        \n",
    "#         xgb_seed = random.randint(0, 2023)\n",
    "#         xgb = XGBClassifier(n_estimators=n_estimators, eta=eta, max_depth=max_depth, subsample=subsample, colsample_bytree=colsample_bytree, random_state=xgb_seed).fit(X_train.values, y_train)\n",
    "        \n",
    "#         val_probs = [probs[1] for probs in xgb.predict_proba(X_val)]\n",
    "#         valid_predictions.loc[val_index, f'XGB_Step_{step}_Fold_{i}'] = val_probs\n",
    "        \n",
    "#         fpr, tpr, thresholds = metrics.roc_curve(y_val, val_probs, pos_label=1)\n",
    "#         auc = metrics.auc(fpr, tpr)\n",
    "#         aucs.append(auc)\n",
    "        \n",
    "#         test_predictions[f'XGB_Step_{step}_Fold_{i}'] = [probs[1] for probs in xgb.predict_proba(processed_test_data[SELECTED_FEATURES])]\n",
    "    \n",
    "#     # Storing the tuning results\n",
    "#     tuning_results['step'].append(step)\n",
    "#     tuning_results['auc'].append(mean(aucs))\n",
    "#     tuning_results['n_estimators'].append(n_estimators)\n",
    "#     tuning_results['eta'].append(eta)\n",
    "#     tuning_results['max_depth'].append(max_depth)\n",
    "#     tuning_results['subsample'].append(subsample)\n",
    "#     tuning_results['colsample_bytree'].append(colsample_bytree)\n",
    "#     tuning_results['skf_seed'].append(skf_seed)\n",
    "#     tuning_results['xgb_seed'].append(xgb_seed)\n",
    "    \n",
    "#     # print(f'Step: {step}  AUC: {mean(aucs)}')\n",
    "\n",
    "# # Saving Predictions and Tuning Results\n",
    "# valid_predictions.to_csv('XGBoost_Valid_Predictions.csv', index=False)\n",
    "# test_predictions.to_csv('XGBoost_Test_Predictions.csv', index=False)\n",
    "\n",
    "# # Finalizing Tuning Results\n",
    "# tuning_results = pd.DataFrame(tuning_results)\n",
    "# tuning_results.sort_values(by='auc', ascending=False, inplace=True)\n",
    "# tuning_results.to_csv('XGBoost_Tuning_Results.csv', index=False)\n",
    "# tuning_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8ba598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specific steps to ensemble (1, 2, 3 regardless of AUC ranking)\n",
    "target_steps = [1, 2, 3]  # Steps you identified as best and balanced\n",
    "num_folds = 10  # From your cv_folds value\n",
    "\n",
    "# Create list of columns for these steps across all folds\n",
    "best_cols = [\n",
    "    f'XGB_Step_{step}_Fold_{fold}'\n",
    "    for step in target_steps\n",
    "    for fold in range(num_folds)\n",
    "]\n",
    "\n",
    "# Calculate mean probability across selected models\n",
    "cv_probs = test_predictions[best_cols].mean(axis=1)\n",
    "\n",
    "# Convert probabilities to binary classes (0/1) with 0.5 threshold\n",
    "cv_classes = (cv_probs >= 0.5).astype(int)\n",
    "\n",
    "# Prepare submission (assuming 'Loan_Status' expects 0/1)\n",
    "sub = sample_submission.copy()\n",
    "sub['Loan_Status'] = cv_classes  # Use binary classes instead of probabilities\n",
    "\n",
    "# Save with descriptive filename\n",
    "sub.to_csv('xgb_steps_1_2_3_ensemble_binary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95fa489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
